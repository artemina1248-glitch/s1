{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0505ca9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/artemuna/d1/nlpm/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/smagnan/1-million-reddit-comments-from-40-subreddits?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71.2M/71.2M [00:02<00:00, 27.4MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/artemuna/.cache/kagglehub/datasets/smagnan/1-million-reddit-comments-from-40-subreddits/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"smagnan/1-million-reddit-comments-from-40-subreddits\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c45f8446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       subreddit                                               body  \\\n",
      "0  gameofthrones  Your submission has been automatically removed...   \n",
      "1            aww  Dont squeeze her with you massive hand, you me...   \n",
      "2         gaming  It's pretty well known and it was a paid produ...   \n",
      "3           news  You know we have laws against that currently c...   \n",
      "4       politics  Yes, there is a difference between gentle supp...   \n",
      "\n",
      "   controversiality  score  \n",
      "0                 0      1  \n",
      "1                 0     19  \n",
      "2                 0      3  \n",
      "3                 0     10  \n",
      "4                 0      1  \n",
      "(1000000, 4)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "path = \"/home/artemuna/.cache/kagglehub/datasets/smagnan/1-million-reddit-comments-from-40-subreddits/versions/1\"\n",
    "\n",
    "# 自动找到第一个 csv\n",
    "csv_files = [f for f in os.listdir(path) if f.endswith(\".csv\")]\n",
    "\n",
    "df = pd.read_csv(os.path.join(path, csv_files[0]))\n",
    "\n",
    "print(df.head())\n",
    "print(df.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0505b213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from functools import lru_cache\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# ---------- 1. Basic person lexicon (you can extend) ----------\n",
    "\n",
    "PERSON_PRONOUNS = {\n",
    "    \"i\", \"you\", \"he\", \"she\", \"we\", \"they\",\n",
    "    \"me\", \"him\", \"her\", \"us\", \"them\",\n",
    "    \"myself\", \"yourself\", \"himself\", \"herself\",\n",
    "    \"ourselves\", \"themselves\"\n",
    "}\n",
    "\n",
    "PERSON_LEXICON = {\n",
    "    # generic human nouns\n",
    "    \"person\", \"people\", \"human\", \"humans\", \"individual\", \"individuals\",\n",
    "    \"man\", \"men\", \"woman\", \"women\",\n",
    "    \"guy\", \"guys\", \"girl\", \"girls\", \"boy\", \"boys\",\n",
    "    \"child\", \"children\", \"kid\", \"kids\",\n",
    "    \"adult\", \"adults\", \"teen\", \"teens\", \"teenager\", \"teenagers\",\n",
    "\n",
    "    # roles / occupations (frequent)\n",
    "    \"teacher\", \"doctor\", \"nurse\", \"driver\", \"customer\",\n",
    "    \"worker\", \"employee\", \"employer\",\n",
    "    \"student\", \"passenger\", \"client\", \"manager\",\n",
    "    \"police\", \"officer\", \"chef\", \"waiter\", \"waitress\",\n",
    "    \"farmer\", \"engineer\", \"scientist\", \"actor\", \"actress\",\n",
    "    \"boss\", \"colleague\", \"partner\", \"friend\", \"buddy\",\n",
    "\n",
    "    # address terms\n",
    "    \"sir\", \"madam\", \"bro\", \"dude\", \"mate\", \"folks\", \"pal\"\n",
    "}\n",
    "\n",
    "\n",
    "# ---------- 2. Helpers using WordNet ----------\n",
    "\n",
    "def _wordnet_lexnames(noun: str):\n",
    "    \"\"\"Return a set of lexnames for the noun from WordNet.\"\"\"\n",
    "    synsets = wn.synsets(noun, pos=wn.NOUN)\n",
    "    return {s.lexname() for s in synsets}\n",
    "\n",
    "\n",
    "def _wordnet_is_person(noun: str) -> bool:\n",
    "    \"\"\"Check whether WordNet sees this noun as a person.\"\"\"\n",
    "    lexnames = _wordnet_lexnames(noun)\n",
    "    if \"noun.person\" in lexnames:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def _wordnet_is_location(noun: str) -> bool:\n",
    "    \"\"\"Check whether WordNet treats this noun as a location.\"\"\"\n",
    "    lexnames = _wordnet_lexnames(noun)\n",
    "    return \"noun.location\" in lexnames\n",
    "\n",
    "\n",
    "def _wordnet_is_artifact_or_object(noun: str) -> bool:\n",
    "    \"\"\"Check whether WordNet treats this noun as an artifact / physical object.\"\"\"\n",
    "    lexnames = _wordnet_lexnames(noun)\n",
    "    # artifact / object / food / animal / plant etc can be treated as non-person objects\n",
    "    object_like = {\n",
    "        \"noun.artifact\",\n",
    "        \"noun.object\",\n",
    "        \"noun.food\",\n",
    "        \"noun.animal\",\n",
    "        \"noun.plant\",\n",
    "        \"noun.body\",\n",
    "        \"noun.substance\"\n",
    "    }\n",
    "    return any(name in lexnames for name in object_like)\n",
    "\n",
    "\n",
    "def _wordnet_is_group_org_like(noun: str) -> bool:\n",
    "    \"\"\"Check whether WordNet lexnames suggest group / org-like concepts.\"\"\"\n",
    "    lexnames = _wordnet_lexnames(noun)\n",
    "    # noun.group often covers groups, organizations, teams, etc.\n",
    "    return \"noun.group\" in lexnames\n",
    "\n",
    "\n",
    "# ---------- 3. Helpers using ConceptNet (cached) ----------\n",
    "\n",
    "CONCEPTNET_API = \"https://api.conceptnet.io/query\"\n",
    "\n",
    "@lru_cache(maxsize=2048)\n",
    "def _conceptnet_is_a_targets(noun: str):\n",
    "    \"\"\"\n",
    "    Query ConceptNet for IsA relations and return a list of target labels.\n",
    "    Cached to avoid repeated HTTP calls.\n",
    "    \"\"\"\n",
    "    term = noun.replace(\" \", \"_\")\n",
    "    url = f\"{CONCEPTNET_API}?node=/c/en/{term}&rel=/r/IsA&limit=20\"\n",
    "    try:\n",
    "        resp = requests.get(url, timeout=2)\n",
    "        if resp.status_code != 200:\n",
    "            return []\n",
    "        data = resp.json()\n",
    "        labels = []\n",
    "        for edge in data.get(\"edges\", []):\n",
    "            end = edge.get(\"end\", {})\n",
    "            label = end.get(\"label\")\n",
    "            if label:\n",
    "                labels.append(label.lower())\n",
    "        return labels\n",
    "    except Exception:\n",
    "        # Fail silently: if ConceptNet is not reachable, just return empty\n",
    "        return []\n",
    "\n",
    "\n",
    "def _conceptnet_class(noun: str):\n",
    "    \"\"\"\n",
    "    Use ConceptNet IsA targets to infer coarse class:\n",
    "    PERSON / OBJECT / LOCATION / ORG / None\n",
    "    \"\"\"\n",
    "    labels = _conceptnet_is_a_targets(noun)\n",
    "    if not labels:\n",
    "        return None\n",
    "\n",
    "    # Directly human-like\n",
    "    human_keywords = {\"person\", \"human\", \"man\", \"woman\", \"boy\", \"girl\", \"people\"}\n",
    "    if any(any(hk in lbl for hk in human_keywords) for lbl in labels):\n",
    "        return \"PERSON\"\n",
    "\n",
    "    # Location-like\n",
    "    loc_keywords = {\"place\", \"city\", \"country\", \"region\", \"location\", \"town\", \"village\"}\n",
    "    if any(any(lk in lbl for lk in loc_keywords) for lbl in labels):\n",
    "        return \"LOCATION\"\n",
    "\n",
    "    # Organization-like\n",
    "    org_keywords = {\"company\", \"organization\", \"institution\", \"university\", \"school\", \"team\"}\n",
    "    if any(any(ok in lbl for ok in org_keywords) for lbl in labels):\n",
    "        return \"ORG\"\n",
    "\n",
    "    # Object-like (device, tool, vehicle, furniture, food, etc.)\n",
    "    obj_keywords = {\n",
    "        \"object\", \"artifact\", \"device\", \"tool\", \"vehicle\", \"furniture\",\n",
    "        \"machine\", \"food\", \"drink\", \"instrument\", \"appliance\"\n",
    "    }\n",
    "    if any(any(ok in lbl for ok in obj_keywords) for lbl in labels):\n",
    "        return \"OBJECT\"\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# ---------- 4. Main classifier (no LLM) ----------\n",
    "\n",
    "def classify_noun_advanced(noun: str) -> str:\n",
    "    \"\"\"\n",
    "    Classify a noun into:\n",
    "        PERSON / OBJECT / LOCATION / ORG / OTHER\n",
    "\n",
    "    Priority:\n",
    "      1. Pronouns and explicit person lexicon\n",
    "      2. WordNet (person / location / artifact / group)\n",
    "      3. ConceptNet IsA\n",
    "      4. Default: OBJECT (if it looks concrete), else OTHER\n",
    "    \"\"\"\n",
    "    if not noun:\n",
    "        return \"OTHER\"\n",
    "\n",
    "    # normalize\n",
    "    token = noun.strip().lower()\n",
    "    if not token:\n",
    "        return \"OTHER\"\n",
    "\n",
    "    # 1) Pronouns\n",
    "    if token in PERSON_PRONOUNS:\n",
    "        return \"PERSON\"\n",
    "\n",
    "    # 2) Person lexicon\n",
    "    if token in PERSON_LEXICON:\n",
    "        return \"PERSON\"\n",
    "\n",
    "    # 3) WordNet-based decisions\n",
    "    if _wordnet_is_person(token):\n",
    "        return \"PERSON\"\n",
    "\n",
    "    if _wordnet_is_location(token):\n",
    "        return \"LOCATION\"\n",
    "\n",
    "    if _wordnet_is_group_org_like(token):\n",
    "        # You may want to distinguish ORG vs generic group, but treat as ORG here\n",
    "        return \"ORG\"\n",
    "\n",
    "    if _wordnet_is_artifact_or_object(token):\n",
    "        # artifact / object / food / animal / body-part etc → treat as OBJECT (non-person)\n",
    "        return \"OBJECT\"\n",
    "\n",
    "    # 4) ConceptNet-based decisions (if available)\n",
    "    cn_class = _conceptnet_class(token)\n",
    "    if cn_class is not None:\n",
    "        return cn_class\n",
    "\n",
    "    # 5) Simple heuristic: if the word looks concrete (no obvious abstract suffix), call it OBJECT\n",
    "    abstract_suffixes = (\"-ism\", \"-ity\", \"-ness\", \"-tion\", \"-sion\", \"-ment\", \"-ship\")\n",
    "    if not any(token.endswith(suf) for suf in abstract_suffixes):\n",
    "        return \"OBJECT\"\n",
    "\n",
    "    # 6) Fallback\n",
    "    return \"OTHER\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
